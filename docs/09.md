# Chapter9 동적 크롤링과 정규 표현식
## 9.1 동적 크롤링이란?
* 동적 데이터 특징
  * 입력, 클릭, 로그인 등의 행위를 했을때 데이터가 바뀜
### 9.1.1 셀레늄 실습하기
* selenium 설치 및 지원 도구 설치
``` 
pip install selenium
pip install webdriver_manager
```
* 설치 확인
``` 
## selenium, webdriver 설치 확인
import selenium
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager

driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))
```
* naver 페이지 크롤링하기
``` 
# naver 페이지 가져오기
driver.get('https://www.naver.com')
driver.page_source[1:1000]

# 뉴스 카테고리 클릭
from selenium.webdriver.common.by import By
driver.find_element(By.LINK_TEXT, value='뉴스').click()

# 뒤로 가기
driver.back()

# 검색창에 검색어 넣기
driver.find_element(By.CLASS_NAME, value='search_input').send_keys('퀀트 투자 포트폴리오 만들기')

# 엔터 이벤트 주기
from selenium.webdriver.common.keys import Keys
driver.find_element(By.CLASS_NAME, value='btn_search').send_keys(Keys.ENTER)
# driver.find_element(By.CLASS_NAME, value='btn_search').click()

# XPATH를 이용해 VIEW 클릭 동작
driver.find_element(By.XPATH, value='//*[@id="lnb"]/div[1]/div/ul/li[2]').click()

# XPATH를 이용해 옵션 클릭 동작
driver.find_element(By.XPATH, value='//*[@id="snb"]/div[1]/div/div[3]/a').click()

# 옵션창 닫기
driver.find_element(By.CLASS_NAME, value='bt_close').click()
driver.find_element(By.XPATH, value='//*[@id="snb"]/div[2]/button').click()

# 옵셩창 열고 최신순 클릭
driver.find_element(By.XPATH, value='//*[@id="snb"]/div[1]/div/div[3]/a').click()
driver.find_element(By.XPATH, value='//*[@id="snb"]/div[2]/ul/li[2]/div/div/a[2]').click()

# page down 기능 수행
driver.find_element(By.TAG_NAME, value='body').send_keys(Keys.PAGE_DOWN)
# driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')

# 스크롤 끝까지 내리기
import time
prev_height = driver.execute_script('return document.body.scrollHeight')

while True:
    driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')
    time.sleep(2)
    
    curr_height = driver.execute_script('return document.body.scrollHeight')
    if curr_height == prev_height:
        break
    prev_height = curr_height
    
# 제목 크롤링
from bs4 import BeautifulSoup
html = BeautifulSoup(driver.page_source, 'lxml')
txt = html.find_all(class_ = 'api_txt_lines total_tit _cross_trigger')
txt_list = [i.get_text() for i in txt]

txt_list[0:10]

# driver 종료
driver.quit()
```
### 9.1.2 셀레늄 명령어 정리
#### 9.1.2.1 브라우저 관련
* webdriver.Chrome() : 브라우저 열기
* driver.close() : 현재 탭 닫기
* driver.quit() : 브라우저 닫기
* driver.back() : 뒤로 가기
* driver.forward() : 앞으로 가기
#### 9.1.2.2 엘리먼트 접근
* By.ID : 태그의 ID 값으로 추출
* By.NAME : 태그의 NAME 값으로 추출
* By.XPATH : 태그의 XPATH 값으로 추출
* By.LINK_TEXT : 링크에 존재하는 텍스트로 추출
* By.TAG_NAME : 태그명으로 추출
* By.CLASS_NAME : 태그의 클래스명으로 추출
* By.CSS_SELECTOR : CSS 선택자로 추출
#### 9.1.2.3 동작
* click() : 엘리먼트를 클릭
* clear() : 텍스트 삭제
* send_keys(text) : 텍스트 입력
* send_keys(Keys.CONTROL + 'v') : 컨트롤 + v 누르기
#### 9.1.2.4 자바스크립트 코드 실행
* execute_script() : 자바스크릭트 코드 실행
* Document : https://selenium-python.readthedocs.io/
## 9.2 정규 표현식
### 9.2.1 정규 표현식을 알아야 하는 이유
* 아래 예문에서 숫자만 뽑아내고 싶다면?
```'동 기업의 매출액은 전년 대비 29.2% 늘어났습니다.'```
``` 
import re

data = '동 기업의 매출액은 전년 대비 29.2% 늘어났습니다.'
re.findall('\d+.\d+%', data)
```
### 9.2.2 메타 문자
* 메타 문자란?
  * 문자가 가진 원래의 의미가 아닌 특별한 용도로 사용되는 문자
  ``` 
  .^$*+?{}[]\|()
  ```
#### 9.2.2.1 문자 클래스([ ])
* 대괄호 안에 포함된 문자들 중 하나와 매치
  * ex) 'apple', 'blueberry', 'coconut'과 ```[ae]``` 매치
    * 'apple' : 매치
    * 'blueberry' : 매치
    * 'coconut' : 매치x
* 대괄호 안의 두 문자 사이에 하이폰(-)을 입력하면 두 문자 사이의 범위를 의미
  * ex) ```[a-e]``` : ```[abcde]``` 동일
  * ex) ```[0-5]``` : ```[012345]``` 동일
* 대괄호 안의 ^는 반대를 의미
  * ex) ```[^abc]``` : a, b, c를 제외한 모든 문자와 매치
* 정규 표현식 숏컷
  * ```\d```
    * 숫자와 매치
    * ```[0-9]```와 동일
  * ```\D```
    * 숫자가 아닌 것과 매치
    * ```[^0-9]```와 동일
  * ```\s```
    * wihtespace 문자와 매치
    * ```[\t\n\r\f\v]```와 동일
  * ```\S```
    * wihtespace 문자가 아닌 것과 매치
    * ```[^\t\n\r\f\v]```와 동일
  * ```\w```
    * 문자+숫자(alphanumeric)와 매치
    * ```[a-z-A-Z0-9]```와 동일
  * ```\W```
    * * 문자+숫자(alphanumeric)가 아닌 것과 매치
    * ```[^a-z-A-Z0-9]```와 동일
#### 9.2.2.2 모든 문자(.)
* Dot(.) 메타 문자는 임의의 한 문자와 매칭
  * ex) 'abe', 'ace', 'abate', 'ae'와 a.e와 매치
    * 'abe' : 매치
    * 'ace' : 매치
    * 'abate' : 매치x
    * 'ae' : 매치x
#### 9.2.2.3 반복문
* ```*``` : 0부터 무한 반복
* ```+``` : 최소 1번 이상 반복
* ```{m ,n}``` : m부터 n까지 반복
* ```?```: 최대 1번 반복, ```{0,1}```과 동일
#### 9.2.2.4 기타 메타 문자
### 9.2.3 정규식을 이용한 문자열 검색
* match()
  * 시작 부분부터 일치하는 패턴을 찾는다
* search()
  * 첫 번째 일치하는 패턴을 찾는다
* findall()
  * 일치하는 모든 패턴을 찾는다
* finditer()
  * 일치하는 모든 패턴을 찾고 반복 가능한 객체를 반환
#### 9.2.3.1 match()
``` 
p = re.compile('[a-z]+')
type(p)

m = p.match('python')
print(m)
m.group()

m = p.match('Use python')
print(m)

m = p.match('uSe python')
print(m)
```
#### 9.2.3.2 search()
``` 
p = re.compile('[a-z]+')

m = p.search('python')
print(m)

m = p.search('Use python')
print(m)
```
#### 9.2.3.3 findall()
``` 
p = re.compile('[a-zA-Z]+')
m = p.findall('Life is too short, You need Python.')
print(m)
```
#### 9.2.3.4 finditer()
``` 
p = re.compile('[a-zA-Z]+')
m = p.finditer('Life is too short, You need Python.')
print(m)
for i in m:
    print(i)
```
### 9.2.4 정규 표현식 연습해 보기
``` 
num = """r\n\t\t\t\t\t\t\t\r\n\t\t\t\t\t\t\t\t15\r\n\t\t\t\t\t\t\t\t23\r\n\t\t\t\t\t\t\t\t29\r\n\t\t\t\t\t\t\t\t34\r\n\t\t\t\t\t\t\t\t40\r\n\t\t\t\t\t\t\t\t44\r\n\t\t\t\t\t\t\t\r\n\t\t\t\t\t\t"""
p = re.compile('[0-9]+')
m = p.findall(num)
print(m)
```

``` 
dt = '> 오늘의 날짜는 2022.12.31 입니다.'

p = re.compile('[0-9]+.[0-9]+.[0-9]+')
p.findall(dt)

p = re.compile('[0-9]+')
m = p.findall(dt)
print(m)
['.'.join(m)]
```
* 오랑 카페 크롤링 하기
``` 
# pip install html5lib
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
from bs4 import BeautifulSoup
import pandas as pd
import re

driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))
driver.get('https://cafe.naver.com/ohrang')

# login

driver.find_element(By.XPATH, value='//*[@id="menuLink5"]').click()

url = 'https://cafe.naver.com/ArticleList.nhn?search.clubid=24797136&search.menuid=5&search.boardtype=L&search.totalCount=151&search.cafeId=24797136&search.page=1'
driver.get(url)
driver.switch_to.frame('cafe_main')
soup = BeautifulSoup(driver.page_source, 'html.parser')

num_list = [num.text for num in soup.findAll("div",{"class":"inner_number"})]
a_title_list = soup.findAll("a",{"class":"article"})
contents_link_list = ['https://cafe.naver.com'+link['href'] for link in a_title_list]
p = re.compile('[\S]')
title_list = [''.join(p.findall(link.text)) for link in a_title_list]
writer_list = [writer.text for writer in soup.findAll("a",{"class":"m-tcol-c"})]
p2 = re.compile('[0-9]+.[0-9]+.[0-9]+')
regdate_list = [p2.findall(reg_date.text) for reg_date in soup.findAll("td",{"class":"td_date"})]

orang_df = pd.DataFrame({
  'code': num_list,
  'title': title_list,
  'link': contents_link_list,
  'date': regdate_list,
  'owner': writer_list
})
```