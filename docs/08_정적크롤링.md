# Chapter8 정적 크롤링 실습
## 8.1 GET과 POST 방식 이해하기
### 8.1.1 GET 방식
### 8.1.2 POST 방식
## 8.2 크롤링 예제
### 8.2.1 명언 크롤링하기
* 명언 크롤링하기 
``` 
import requests as rq

url = 'https://quotes.toscrape.com/'
quote = rq.get(url)

print(quote)
type(quote)
quote.content[:1000]
```
* BeautifulSoup 라이브러리 이용하기
``` 
# pip install beautifulsoup4
from bs4 import BeautifulSoup

quote_html = BeautifulSoup(quote.content, 'html.parser')
quote_html.head()
```
#### 8.2.1.1 find() 함수를 이용한 크롤링
* find_all() 함수를 이용해 명언 뽑아내기
``` 
quote_div = quote_html.find_all('div', class_='quote')
quote_div

quote_span = quote_div[0].find_all('span', class_='text')
quote_span
quote_span[0].text

quote_span = quote_div[1].find_all('span', class_='text')
quote_span
quote_span[0].text

quote_span = quote_div[2].find_all('span', class_='text')
quote_span
quote_span[0].text

quote_div = quote_html.find_all('div', class_='quote')
[quote.find_all('span', class_='text')[0].text for quote in quote_div]
```
#### 8.2.1.2 select() 함수를 이용한 크롤링
* select() 함수를 이용해 명언 뽑아내기
``` 
quote_text = quote_html.select('div.quote > span.text')
quote_text

[quote.text for quote in quote_text]
```
* select() 함수를 이용해 말한 사람 뽑아내기
``` 
quote_author = quote_html.select('div.quote > span > small.author')
[author.text for author in quote_author]
```
* select() 함수를 이용해 말한 사람 정보 뽑아내기
``` 
quote_link = quote_html.select('div.quote > span > a')
[link['href'] for link in quote_link]
['https://quotes.toscrape.com'+link['href'] for link in quote_link]
```
#### 8.2.1.3 모든 페이지 데이터 크롤링하기
``` 
import requests as rq
from bs4 import BeautifulSoup
import time

text_list = []
author_list = []
infor_list = []

for i in range(1, 100):
    url = f'https://quotes.toscrape.com/page/{i}/'
    quote = rq.get(url)
    quote_html = BeautifulSoup(quote.content, 'html.parser')

    quote_text = quote_html.select('div.quote > span.text')
    quote_text_list = [i.text for i in quote_text]

    quote_author = quote_html.select('div.quote > span > small.author')
    quote_author_list = [i.text for i in quote_author]
    
    quote_link = quote_html.select('div.quote > span > a')
    qutoe_link_list = ['https://quotes.toscrape.com' + i['href'] for i in quote_link]

    if len(quote_text_list) > 0:
        text_list.extend(quote_text_list)
        author_list.extend(quote_author_list)        
        infor_list.extend(qutoe_link_list)        
        time.sleep(1)

    else:
        break
```
### 8.2.2 금융 속보 크롤링
``` 
import requests as rq
from bs4 import BeautifulSoup

url = 'https://finance.naver.com/news/news_list.nhn?mode=LSS2D&section_id=101&section_id2=258'
data = rq.get(url)
html = BeautifulSoup(data.content, 'html.parser')
html_select = html.select('dl > dd.articleSubject > a')

[subject['title'] for subject in html_select]
```
### 8.2.3 표 크롤링하기
``` 
# pip install lxml
import pandas as pd

url = 'https://en.wikipedia.org/wiki/List_of_countries_by_stock_market_capitalization'
tbl = pd.read_html(url)

tbl[0].head()
```
### 8.2.4 기업 공식 채널에서 오늘의 공시 불러오기
``` 
import requests as rq
from bs4 import BeautifulSoup
import pandas as pd

url = 'https://kind.krx.co.kr/disclosure/todaydisclosure.do'
payload = {
    'method': 'searchTodayDisclosureSub',
    'currentPageSize': '15',
    'pageIndex': '1',
    'orderMode': '0',
    'orderStat': 'D',
    'forward': 'todaydisclosure_sub',
    'chose': 'S',
    'todayFlag': 'N',
    'selDate': '2023-05-19'
}

data = rq.post(url, data=payload)
html = BeautifulSoup(data.content, 'html.parser')

# print(html)

tbl = pd.read_html(html.prettify())
tbl[0].head()
```